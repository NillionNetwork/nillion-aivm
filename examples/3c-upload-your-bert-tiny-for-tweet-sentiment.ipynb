{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIVM Tutorial: Uploading Custom Bert Tiny Models to AIVM\n",
    "\n",
    "This tutorial demonstrates how to upload and use custom models with the AIVM (AI Virtual Machine) system, focusing on a BERT-tiny model for SMS spam detection.\n",
    "\n",
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to proxy localhost:50050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import time\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import aivm_client as aic # Import the Nillion-AIVM client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the required Python libraries for:\n",
    "- Neural network operations (`torch`)\n",
    "- Dataset handling (`torchvision`)\n",
    "- Visualization (`matplotlib`)\n",
    "- AIVM client interface (`aivm_client`)\n",
    "\n",
    "## Model Upload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"BertTinySentimentTwitter\" # Name of the model to be used\n",
    "aic.upload_bert_tiny_model(\"./twitter_bert_tiny.onnx\", MODEL_NAME) # Upload the model to the server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell demonstrates how to upload a custom model to the AIVM server:\n",
    "1. Define a unique name for the model\n",
    "2. Upload an ONNX-format model file using the AIVM client\n",
    "3. The model will be available for encrypted inference after upload\n",
    "\n",
    "## IMDB Sentiment Classification Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_positive_inputs = aic.tokenize(\"This movie was absolutely fantastic! I loved every minute of it.\",)\n",
    "encrypted_positive_inputs = aic.BertTinyCryptensor(tokenized_positive_inputs[0], tokenized_positive_inputs[1])\n",
    "\n",
    "tokenized_negative_inputs = aic.tokenize(\"Terrible waste of time. The plot made no sense and the acting was awful.\")\n",
    "encrypted_negative_inputs = aic.BertTinyCryptensor(tokenized_negative_inputs[0], tokenized_negative_inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell prepares the input text for spam detection:\n",
    "1. Tokenizes example messages using BERT tokenization\n",
    "2. Creates an encrypted tensor from the tokenized input\n",
    "3. The second message is a typical spam SMS example\n",
    "\n",
    "## Encrypted Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review prediction:  tensor([[-1.9535, -0.9669,  3.6545]])\n",
      "Negative review prediction:  tensor([[ 2.4873, -0.4958, -1.9657]])\n",
      "CPU times: user 14.6 ms, sys: 11.3 ms, total: 25.9 ms\n",
      "Wall time: 6.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_positive = aic.get_prediction(encrypted_positive_inputs, MODEL_NAME)\n",
    "result_negative = aic.get_prediction(encrypted_negative_inputs, MODEL_NAME)\n",
    "\n",
    "print(\"Positive review prediction: \", result_positive)\n",
    "print(\"Negative review prediction: \", result_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell:\n",
    "1. Performs encrypted inference using our uploaded custom model\n",
    "2. Measures the execution time\n",
    "3. Returns the raw model output (logits)\n",
    "\n",
    "Output shows:\n",
    "- CPU usage statistics\n",
    "- Wall time for inference\n",
    "- Tensor with prediction scores similar to `tensor([[-0.9099,  0.9210]])`\n",
    "\n",
    "\n",
    "\n",
    "## Result Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review prediction:  Negative\n",
      "Negative review prediction:  Positive\n"
     ]
    }
   ],
   "source": [
    "sentiment = lambda x: \"Negative\" if torch.argmax(x) == 0 else \"Positive\" if torch.argmax(x) == 2 else \"Neutral\"\n",
    "print(\"Positive review prediction: \", sentiment(result_positive))\n",
    "print(\"Negative review prediction: \", sentiment(result_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell interprets the model's prediction:\n",
    "- Converts numerical output to a binary classification\n",
    "- Tells whether a review is positive or negative\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Model Management**\n",
    "   - Custom models can be uploaded to AIVM in ONNX format\n",
    "   - Models are referenced by unique names\n",
    "   - Uploaded models persist on the server for future use\n",
    "\n",
    "2. **Privacy-Preserving Inference**\n",
    "   - All inference is performed on encrypted data\n",
    "   - Model weights and architecture remain secure\n",
    "   - Results are only decrypted at the client side\n",
    "\n",
    "3. **Performance Considerations**\n",
    "   - Encrypted computation introduces some overhead\n",
    "   - Wall time measurements help evaluate real-world performance\n",
    "   - System is optimized for practical use cases\n",
    "\n",
    "4. **Integration Workflow**\n",
    "   1. Upload model\n",
    "   2. Prepare and encrypt inputs\n",
    "   3. Perform inference\n",
    "   4. Interpret results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
